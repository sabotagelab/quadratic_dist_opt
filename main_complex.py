import argparse
import matplotlib.pyplot as plt
import mpl_toolkits.mplot3d.art3d as art3d
import numpy as np

from objective import Objective
from generate_trajectories import Quadrocopter
from generate_trajectories import generate_agent_states, generate_init_traj_quad


if __name__ == "__main__":
    np.random.seed(42)

    parser = argparse.ArgumentParser(description='Centralized Optimization')
    parser.add_argument('--N', type=int, default=3)
    parser.add_argument('--H', type=int, default=5)
    parser.add_argument('--alpha', type=float, default=1)
    parser.add_argument('--beta', type=float, default=10)
    parser.add_argument('--gamma', type=float, default=1)
    parser.add_argument('--kappa', type=float, default=.1)
    parser.add_argument('--eps_bounds', type=float, default=1)
    
    parser.add_argument('--ro', type=float, default=0.5)
    parser.add_argument('--co', type=float, default=3)
    parser.add_argument('--rg', type=float, default=0.5)
    parser.add_argument('--cg', type=float, default=5)
    parser.add_argument('--Ubox', type=float, default=50)  # NEED A LARGER UBOX THAN IN THE SIMPLE EXAMPLE
    parser.add_argument('--iter', type=int, default=1000)

    parser.add_argument('--Tf', type=int, default=1)

    args = parser.parse_args()
    print(args)

    N = args.N  # number of agents
    alpha = args.alpha   # parameter for fairness constraint
    beta = args.beta    # parameter for weighting of obstacle avoidance constraint
    gamma = args.gamma   # parameter for smoothmin in calculating obstacle avoidance constraint
    kappa = args.kappa   # parameter for weighting change in epsilon for local problem
    eps_bounds = args.eps_bounds  # bounds for eps in an iteration

    ro = args.ro  # radius of circle
    co = np.array([args.co, args.co, args.co])  # center of circle
    rg = args.rg
    cg = np.array([args.cg, args.cg, 0])
    obstacles = {'center': co, 'radius': ro}
    target = {'center': cg, 'radius': rg}
    Ubox = args.Ubox  # box constraint

    H = args.H

    Tf = args.Tf
    
    # SET INITIAL POSITIONS AND STATES
    init_pos = [np.array([-3, 3, 0]), np.array([-3, -3, 0]), np.array([3, -3, 0])]
    init_states = []
    for i in range(N):
        s = [init_pos[i]]
        s.append(np.array([0, 0, 0]))  # velo
        init_states.append(np.array(s).flatten())

    # GENERATE INITIAL "CONTROL INPUTS" AND TRAJECTORIES
    init_u = []
    init_traj = []
    for i in range(N):
        traj_pos, traj_accel = generate_init_traj_quad(init_pos[i], cg, H, Tf=Tf)
        init_u.append(traj_accel)
        init_traj.append(traj_pos)
    init_u = np.array(init_u)
    # print('Inputs')
    # print(init_u)

    # PLOT INITIAL TRAJECTORIES AS GENERATED BY ANDREAS CODE
    # fig = plt.figure()
    # ax = fig.add_subplot(projection='3d')
    # for i in range(N):
    #     # print("generated trajectory {}".format(i))
    #     # print(init_traj[i])
    #     ax.scatter(init_traj[i][:,0], init_traj[i][:,1], init_traj[i][:,2], label=i)
    # plt.show()

    # PLOT INITIAL TRAJECTORIES FROM "CONTROL INPUTS"
    init_trajectories = []
    fig = plt.figure()
    ax = fig.add_subplot(projection='3d')
    times = np.linspace(0, Tf, H)
    for i in range(N):
        states, traj = generate_agent_states(init_u[i], init_states[i], init_pos[i], model=Quadrocopter, dt=Tf/H*1.5)
        ax.scatter(traj[:,0], traj[:,1], traj[:,2], label=i)
        init_trajectories.append(traj)
        # print("States {}".format(i))
        # print(states)
        # print("Inputs {}".format(i))
        # print(init_u[i])
        # print("Traj {}".format(i))
        # print(traj)
    obs = plt.Circle((co[0], co[1]), ro, fill=True, alpha=0.2, color='red')
    ax.add_patch(obs)
    art3d.pathpatch_2d_to_3d(obs, z=co[2])
    goal = plt.Circle((cg[0], cg[1]), rg, fill=True, alpha=0.2, color='green')
    ax.add_patch(goal)
    art3d.pathpatch_2d_to_3d(goal, z=cg[2])
    plt.show()


    # INIT SOLVER
    system_model = Quadrocopter
    control_input_size = 3
    system_model_config = (Quadrocopter, control_input_size)

    n = N*H*control_input_size
    Q = np.random.randn(n, n)   # variable for quadratic objective
    Q = Q.T @ Q
    obj = Objective(N, H, system_model_config, init_states, init_pos, obstacles, target, Q, alpha, beta, gamma, kappa, eps_bounds, Ubox, dt=Tf/H*1.5)

    # METRICS FOR INITIAL TRAJECTORY
    print('Initial Obj {}'.format(obj.central_obj(init_u.flatten())))
    print('Initial Total Energy Cost (Lower is better)')
    print(obj.quad(init_u.flatten()))
    print('Initial Fairness (Close to 0 is better)')
    print(obj.fairness(init_u.flatten()))
    print('Initial Obstacle Avoidance Cost (More Negative Is Better)')
    print(obj.obstacle(init_u.flatten()))

    # SOLVE USING CENTRAL
    final_obj, final_u = obj.solve_central(init_u, steps=10)
    print('Central Final Obj {}'.format(final_obj))
    print('Solved Inputs')
    final_u = final_u.reshape(N, H, control_input_size)
    print(final_u)

    # METRICS FOR FINAL TRAJECTORY AFTER SOLVING CENTRAL PROBLEM
    print('Central Final Total Energy Cost (Lower is better)')
    print(obj.quad(final_u.flatten()))
    print('Central Final Fairness (Close to 0 is better)')
    print(obj.fairness(final_u.flatten()))
    print('Central Final Obstacle Avoidance Cost (More Negative Is Better)')
    print(obj.obstacle(final_u.flatten()))

    # PLOT FINAL TRAEJECTORIES FROM CONTROL INPUTS
    final_trajectories = []
    fig = plt.figure()
    ax = fig.add_subplot(projection='3d')
    times = np.linspace(0, Tf, H)
    for i in range(N):
        _, traj = generate_agent_states(final_u[i], init_states[i], init_pos[i], model=Quadrocopter, dt=1.0/H*1.5)
        ax.scatter(traj[:,0], traj[:,1], traj[:,2], label=i)
        final_trajectories.append(traj)
    obs = plt.Circle((co[0], co[1]), ro, fill=True, alpha=0.2, color='red')
    ax.add_patch(obs)
    art3d.pathpatch_2d_to_3d(obs, z=co[2])
    goal = plt.Circle((cg[0], cg[1]), rg, fill=True, alpha=0.2, color='green')
    ax.add_patch(goal)
    art3d.pathpatch_2d_to_3d(goal, z=cg[2])
    plt.show()

    # SOLVE USING DISTRIBUTED OPTIMIZATION
    final_u, local_sols, fairness = obj.solve_distributed(init_u, steps=10, dyn='quad')
    print('Distributed Final Obj {}'.format(obj.central_obj(final_u.flatten())))
    print(obj.central_obj(final_u.flatten()))
    print('Solved Inputs')
    print(final_u)

    # METRICS FOR FINAL TRAJECTORY AFTER SOLVING DISTRIBUTED PROBLEM
    print('Final Total Energy Cost (Lower is better)')
    print(obj.quad(final_u.flatten()))
    print('Final Fairness (Close to 0 is better)')
    print(obj.fairness(final_u.flatten()))
    print('Final Obstacle Avoidance Cost (More Negative Is Better)')
    print(obj.obstacle(final_u.flatten()))

    # PLOT FINAL TRAEJECTORIES FROM CONTROL INPUTS
    final_trajectories = []
    fig = plt.figure()
    ax = fig.add_subplot(projection='3d')
    times = np.linspace(0, Tf, H)
    for i in range(N):
        _, traj = generate_agent_states(final_u[i], init_states[i], init_pos[i], model=Quadrocopter, dt=1.0/H*1.5)
        ax.scatter(traj[:,0], traj[:,1], traj[:,2], label=i)
        final_trajectories.append(traj)
    obs = plt.Circle((co[0], co[1]), ro, fill=True, alpha=0.2, color='red')
    ax.add_patch(obs)
    art3d.pathpatch_2d_to_3d(obs, z=co[2])
    goal = plt.Circle((cg[0], cg[1]), rg, fill=True, alpha=0.2, color='green')
    ax.add_patch(goal)
    art3d.pathpatch_2d_to_3d(goal, z=cg[2])
    plt.show()
